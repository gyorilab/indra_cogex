{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48607914-1fbc-4098-a30f-41a611afb8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sangeethavempati/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from indra_cogex.sources.odinson.grammars import Rule\n",
    "from indra_cogex.sources.odinson.client import process_rules\n",
    "import gilda\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from gilda.process import normalize\n",
    "from tqdm.auto import tqdm\n",
    "from pyobo.gilda_utils import get_gilda_terms\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import textwrap\n",
    "import random\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeaba167-7050-424d-8fc6-30ff150133cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[UBERON] mapping: 100%|██████████████████| 14.5k/14.5k [00:00<00:00, 222kname/s]\n",
      "[UBERON] mapping: 100%|██████████████| 9.84k/9.84k [00:00<00:00, 70.9ksynonym/s]\n",
      "[fma] mapping: 100%|█████████████████████| 79.0k/79.0k [00:00<00:00, 249kname/s]\n",
      "[fma] mapping: 100%|█████████████████| 29.8k/29.8k [00:00<00:00, 87.9ksynonym/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gilda.grounder.Grounder at 0x2acf17350>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reformat terms\n",
    "df = pd.read_csv('spine.tsv', sep = '\\t', header = None, names = ['name', 'id'])\n",
    "#for each id, make a list of the corresponding brain regions\n",
    "id_to_names = defaultdict(list)\n",
    "for name,identifier in df.values:\n",
    "    id_to_names[identifier].append(name)\n",
    "terms = []\n",
    "#label terms with the same id as synonyms\n",
    "for identifier, names in id_to_names.items():\n",
    "    #label everything except the first as synonyms\n",
    "    name,*synonyms = names\n",
    "    term = gilda.term.Term(\n",
    "                norm_text=normalize(name),\n",
    "                text=name,\n",
    "                db=\"spine\",\n",
    "                id=identifier,\n",
    "                entry_name=name,\n",
    "                status=\"name\",\n",
    "                source=\"spine\",\n",
    "            )\n",
    "    terms.append(term)\n",
    "    for synonym in synonyms:\n",
    "        term = gilda.term.Term(\n",
    "                norm_text=normalize(synonym),\n",
    "                text=synonym,\n",
    "                db=\"spine\",\n",
    "                id=identifier,\n",
    "                entry_name=name,\n",
    "                status=\"synonym\",\n",
    "                source=\"spine\",\n",
    "            )\n",
    "        terms.append(term)\n",
    "terms.extend(get_gilda_terms('UBERON'))\n",
    "terms.extend(get_gilda_terms('fma'))\n",
    "#terms.extend(get_gilda_terms('ncit'))\n",
    "grounder = gilda.Grounder(terms)\n",
    "grounder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a013c2b-b1c4-4884-a2eb-768b023bdb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2916"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "#adding in adverbs and directions\n",
    "'''\n",
    "noun_inputs = [\"medial\",\"lateral\",\"superior\",\"posterior\",\"dorsal\",\"ipsilateral\",\"efferent\",\"outer\",\"central\",\"caudal\",\"afferent\",\"contralateral\",\n",
    "              \"ventral\",\"frontal\",\"terminal\",\"rostral\",\"inner\",\"anterior\",\"ascending\",\"peripheral\",\"Descending\",\"adjacent\",\n",
    "              \"secondary afferent\",\"auditory ascending\",\"Afferent\",\"cortical\",\"spinal\",\"primary\",\"sensory\",\"motor\",\"projection\",\"visual\",\"auditory\",\"sensorimotor\",\"basal\",\"periaqueductal\",\"limbic\"\n",
    "              \"tectal\",\"dentate\",\"entorhinal\",\"subcortical\",\"somatosensory\",\"olfactory\",\"isthmic\",\"cingulate\",\"orbitofrontal\",\n",
    "              \"Intrahemispheric\",\"geniculocortical\",\"associational\",\"primary mechanosensory\",\"organum\",\"septal\",\"Tectal\",\"dense cortical\",\n",
    "              \"Cortical\",\"mesopontine tegmental\",\"primary trigeminal\",\"hypoglossal\",\"occipital\",\"parahippocampal\",\"cerebellar\",\"major\",\n",
    "              \"intramedullary\",\"corticofugal\",\"suprageniculate\",\"parvocellular\",\"paraventricular\",\"cortico/-/cortical\",\"temporal\",\"lateral nuclei\",\n",
    "              \"centralis\",\"sympatho/-/excitatory\",\"reticulospinal\",\"vestibulospinal\",\"neocortical\",\"reticulospinal\",\"retinofugal\",\"subthalamic\",\n",
    "              \"contralateral homologous\",\"neural\",\"trigeminal\",\"vagus\",\"glossopharyngeal\",\"preganglionic\",\"ophthalmic\",\"vestibular primary\",\n",
    "              \"perirhinal\",\"maxillar\",\"postrhinal\",\"intrahemispheric\",\"pretectal\",\"hypothalamic\",\"Auditory\",\"rubro/-/spinal\",\"posterolateral\"\n",
    "              \"reticulata\",\"pre/-/frontal\",\"preoptic\",\"Pretectal\",\"rubral\",\"cortico/-/\",\"mossy\",\"Spinal\",\"Retinal\",\"amygdalostriatal\",\n",
    "              \"interhemispheric\",\"intercollicular\",\"locus\",\"Associational\",\"lateralis\",\"vestibular\",\"caudolateral\",\"multipolar\",\n",
    "              \"retinogeniculocortical\"]\n",
    "'''\n",
    "\n",
    "#adding in adverbs and directions\n",
    "directions = (\"medial|lateral|superior|posterior|dorsal|ipsilateral|efferent|outer|central|caudal|afferent|contralateral|ventral|frontal|\"\n",
    "              \"terminal|rostral|inner|anterior|ascending|peripheral|Descending|adjacent|secondary afferent|auditory ascending|Afferent\")\n",
    "\n",
    "advb = (\"cortical|spinal|primary|sensory|motor|projection|visual|auditory|sensorimotor|basal|periaqueductal|limbic|tectal|dentate|\"\n",
    "        \"entorhinal|subcortical|somatosensory|olfactory|isthmic|cingulate|orbitofrontal|Intrahemispheric|geniculocortical|associational|\"\n",
    "        \"primary mechanosensory|organum|septal|Tectal|dense cortical|Cortical|mesopontine tegmental|primary trigeminal|hypoglossal|\"\n",
    "        \"occipital|parahippocampal|cerebellar|major|intramedullary|corticofugal|suprageniculate|parvocellular|paraventricular|\"\n",
    "        \"cortico/-/cortical|temporal|lateral nuclei|centralis|sympatho/-/excitatory reticulospinal|vestibulospinal|neocortical|\"\n",
    "        \"reticulospinal|retinofugal|subthalamic|contralateral homologous|neural|trigeminal|vagus|glossopharyngeal|preganglionic|\"\n",
    "        \"ophthalmic|vestibular primary|perirhinal|maxillar|postrhinal|intrahemispheric|pretectal|hypothalamic|Auditory|rubro/-/spinal|\"\n",
    "        \"posterolateral|reticulata|pre/-/frontal|preoptic|Pretectal|rubral|cortico/-/|mossy|Spinal|Retinal|amygdalostriatal|\"\n",
    "        \"interhemispheric|intercollicular|locus|Associational|lateralis|vestibular|caudolateral|multipolar|retinogeniculocortical\")\n",
    "\n",
    "#add all noun cases to set, loop through, do either nouncase with np, np with nouncase, or nouncase with nouncase\n",
    "#{}* ensures that advb or direction inserted is optional, includes cases where neither is found\n",
    "noun_case_f = [\"{}* (?<region> [chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])\",\n",
    "              \"both {}* (?<region> [chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP]) and {}* ([chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])\",\n",
    "              \"both {}* ([chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP]) and {}* (?<region> [chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])\",\n",
    "              \"{}* (?<region> [chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP]) and {}* ([chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])\",\n",
    "              \"{}* ([chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP]) and {}* (?<region> [chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])\",\n",
    "              \"{}* (?<region> [chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])/,/ {}* ([chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])\",\n",
    "              \"{}* ([chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])/,/ {}* (?<region> [chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])\",\n",
    "              \"{}* (?<region> [chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])/,/ and {}* ([chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])\",\n",
    "              \"{}* ([chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])/,/ and {}* (?<region> [chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])\"]\n",
    "\n",
    "#namedx_np_f = \"{}* (?<region> [chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])\"\n",
    "lemmas = [\"project\", \"connect\", \"pathway\"]\n",
    "fromto = [\">nmod_from\",\">nmod_of\",\">nmod_to\"]\n",
    "#rule_combos = [namedx_np_f,noun_case_f]\n",
    "noun_inputs = [directions,advb]\n",
    "\n",
    "\n",
    "#rule generation\n",
    "#binary_rules = []\n",
    "\n",
    "def create_rules(noun_type1,noun_input1,lemma,word,noun_type2,noun_input2):\n",
    "\n",
    "    #fromto_portion = [\">nmod_{}\".format(word) for word in fromto]\n",
    "    #lemma_portion = [\"[lemma={}]\".format(term) for term in lemmas]\n",
    "    #entity_types = ['namedx','case']\n",
    "    #create two word cases for BRs with noun_input of either advb or direction\n",
    "    if noun_type1 == \"{}* (?<region> [chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])\":\n",
    "        entity1 = noun_type1.format(noun_input1)\n",
    "    else:\n",
    "        entity1 = noun_type1.format(noun_input1,noun_input1)\n",
    "        \n",
    "    if noun_type2 == \"{}* (?<region> [chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])\":\n",
    "        entity2 = noun_type2.format(noun_input2)\n",
    "    else:\n",
    "        entity2 = noun_type2.format(noun_input2,noun_input2)\n",
    "\n",
    "    #create rules with whatever the current case is of each entity\n",
    "    rule_set = [\"{} [lemma={}] {} {}\".format(entity1,lemma,word,entity2),\n",
    "                \"{} [] [] [] [lemma={}] {} {}\".format(entity1,lemma,word,entity2),\n",
    "                \"[lemma={}] >nmod_from {} >nmod_to {}\".format(lemma,entity1,entity2),\n",
    "                \"[lemma={}] >nmod_of {} >nmod_with {}\".format(lemma,entity1,entity2),\n",
    "                \"[lemma={}] [] [] [] [] {} >nmod_to {}\".format(lemma,entity1,entity2),\n",
    "                \"[lemma={}] >nmod_to {} >nmod_from [] [] {}\".format(lemma,entity1,entity2),\n",
    "                \"{} [] [] [] [lemma={}] {} {}\".format(entity1,lemma,word,entity2),\n",
    "                \"{} [lemma=afferent] {} {}\".format(entity1,word,entity2),\n",
    "                \"{} which [] [] [lemma={}] >nmod_to {}\".format(entity1,lemma,entity2)]\n",
    "    return rule_set\n",
    "    \n",
    "    \n",
    "binary_rules = [create_rules(noun_type1,noun_input1,lemma,word,noun_type2,noun_input2) for noun_type1,noun_input1,lemma,word,noun_type2,noun_input2 in product(noun_case_f,noun_inputs,lemmas,fromto,noun_case_f,noun_inputs)]   \n",
    "\n",
    "len(binary_rules)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a1a9714-5ba9-4036-afa9-18b1a55961ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[lemma=/project|connect|pathway/]\n",
      "1728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor rule in binary_rules:\\n    print(rule)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#adding in adverbs and directions\n",
    "directions = (\"medial|lateral|superior|posterior|dorsal|ipsilateral|efferent|outer|central|caudal|afferent|contralateral|ventral|frontal|\"\n",
    "              \"terminal|rostral|inner|anterior|ascending|peripheral|Descending|adjacent|secondary afferent|auditory ascending|Afferent\")\n",
    "\n",
    "advb = (\"cortical|spinal|primary|sensory|motor|projection|visual|auditory|sensorimotor|basal|periaqueductal|limbic|tectal|dentate|\"\n",
    "        \"entorhinal|subcortical|somatosensory|olfactory|isthmic|cingulate|orbitofrontal|Intrahemispheric|geniculocortical|associational|\"\n",
    "        \"primary mechanosensory|organum|septal|Tectal|dense cortical|Cortical|mesopontine tegmental|primary trigeminal|hypoglossal|\"\n",
    "        \"occipital|parahippocampal|cerebellar|major|intramedullary|corticofugal|suprageniculate|parvocellular|paraventricular|\"\n",
    "        \"cortico/-/cortical|temporal|lateral nuclei|centralis|sympatho/-/excitatory reticulospinal|vestibulospinal|neocortical|\"\n",
    "        \"reticulospinal|retinofugal|subthalamic|contralateral homologous|neural|trigeminal|vagus|glossopharyngeal|preganglionic|\"\n",
    "        \"ophthalmic|vestibular primary|perirhinal|maxillar|postrhinal|intrahemispheric|pretectal|hypothalamic|Auditory|rubro/-/spinal|\"\n",
    "        \"posterolateral|reticulata|pre/-/frontal|preoptic|Pretectal|rubral|cortico/-/|mossy|Spinal|Retinal|amygdalostriatal|\"\n",
    "        \"interhemispheric|intercollicular|locus|Associational|lateralis|vestibular|caudolateral|multipolar|retinogeniculocortical\")\n",
    "\n",
    "#add all noun cases to set, loop through, do either nouncase with np, np with nouncase, or nouncase with nouncase\n",
    "noun_case_f = [\"both {}* (?<region> [chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP]) and {}* ([chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])\",\n",
    "              \"both {}* ([chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP]) and {}* (?<region> [chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])\",\n",
    "              \"{}* (?<region> [chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP]) and {}* ([chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])\",\n",
    "              \"{}* ([chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP]) and {}* (?<region> [chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])\",\n",
    "              \"{}* (?<region> [chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])/,/ {}* ([chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])\",\n",
    "              \"{}* ([chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])/,/ {}* (?<region> [chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])\",\n",
    "              \"{}* (?<region> [chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])/,/ and {}* ([chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])\",\n",
    "              \"{}* ([chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])/,/ and {}* (?<region> [chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])\"]\n",
    "\n",
    "namedx_np_f = \"{}* (?<region> [chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])\"\n",
    "#projection = \"[lemma=/project|connect|pathway/]\"\n",
    "#projection = \"[lemma=project|lemma=connect|lemma=pathway]\"\n",
    "#projection = \"[lemma=project|lemma=connect|lemma=pathway]\"\n",
    "#fromto = \"from|of|to\"\n",
    "\n",
    "'''\n",
    "projection_lemmas = [\"project\", \"connect\", \"pathway\"]\n",
    "projection = \"[lemma=/{}/]\".format(\"|\".join(projection_lemmas))\n",
    "print(projection)\n",
    "'''\n",
    "lemmas = [\"project\", \"connect\", \"pathway\"]\n",
    "\n",
    "#not included\n",
    "region_abbrev = \"\"\n",
    "\n",
    "#rule generation\n",
    "binary_rules = []\n",
    "for fromto in [\"from\", \"of\", \"to\"]:\n",
    "#    for projection in [\"[lemma=project]\", \"[lemma=connect]\", \"[lemma=pathway]\"]:\n",
    "        for case in noun_case_f:\n",
    "            rules = [\"{} {} >nmod_{} {}\".format(namedx_np_f.format(advb),projection,fromto,namedx_np_f.format(advb)),\n",
    "                     \"{} {} >nmod_{} {}\".format(namedx_np_f.format(directions),projection,fromto,namedx_np_f.format(directions)),                         \n",
    "                     \"{} {} >nmod_{} {}\".format(case.format(advb,advb),projection,fromto,namedx_np_f.format(advb)),\n",
    "                     \"{} {} >nmod_{} {}\".format(case.format(directions,directions),projection,fromto,namedx_np_f.format(directions)),                         \n",
    "                     \"{} {} >nmod_{} {}\".format(namedx_np_f.format(advb),projection,fromto,case.format(advb,advb)),\n",
    "                     \"{} {} >nmod_{} {}\".format(namedx_np_f.format(directions),projection,fromto,case.format(directions,directions)),                         \n",
    "                     \"{} {} >nmod_{} {}\".format(case.format(advb,advb),projection,fromto,case.format(advb,advb)),\n",
    "                     \"{} {} >nmod_{} {}\".format(case.format(directions,directions),projection,fromto,case.format(directions,directions)),                         \n",
    "                     \"{} [] [] [] {} >nmod_{} {}\".format(namedx_np_f.format(advb),projection,fromto,namedx_np_f.format(advb)),\n",
    "                     \"{} [] [] [] {} >nmod_{} {}\".format(namedx_np_f.format(directions),projection,fromto,namedx_np_f.format(directions)),\n",
    "                     \"{} [] [] [] {} >nmod_{} {}\".format(case.format(advb,advb),projection,fromto,namedx_np_f.format(advb)),\n",
    "                     \"{} [] [] [] {} >nmod_{} {}\".format(case.format(directions,directions),projection,fromto,namedx_np_f.format(directions)),\n",
    "                     \"{} [] [] [] {} >nmod_{} {}\".format(namedx_np_f.format(advb),projection,fromto,case.format(advb,advb)),\n",
    "                     \"{} [] [] [] {} >nmod_{} {}\".format(namedx_np_f.format(directions),projection,fromto,case.format(directions,directions)),\n",
    "                     \"{} [] [] [] {} >nmod_{} {}\".format(case.format(advb,advb),projection,fromto,case.format(advb,advb)),\n",
    "                     \"{} [] [] [] {} >nmod_{} {}\".format(case.format(directions,directions),projection,fromto,case.format(directions,directions)),\n",
    "                     \"{} >nmod_from {} >nmod_to {}\".format(projection,namedx_np_f.format(advb),namedx_np_f.format(advb)),\n",
    "                     \"{} >nmod_from {} >nmod_to {}\".format(projection,namedx_np_f.format(directions),namedx_np_f.format(directions)),\n",
    "                     \"{} >nmod_from {} >nmod_to {}\".format(projection,case.format(advb,advb),namedx_np_f.format(advb)),\n",
    "                     \"{} >nmod_from {} >nmod_to {}\".format(projection,case.format(directions,directions),namedx_np_f.format(directions)),\n",
    "                     \"{} >nmod_from {} >nmod_to {}\".format(projection,namedx_np_f.format(advb),case.format(advb,advb)),\n",
    "                     \"{} >nmod_from {} >nmod_to {}\".format(projection,namedx_np_f.format(directions),case.format(directions,directions)),\n",
    "                     \"{} >nmod_from {} >nmod_to {}\".format(projection,case.format(advb,advb),case.format(advb,advb)),\n",
    "                     \"{} >nmod_from {} >nmod_to {}\".format(projection,case.format(directions,directions),case.format(directions,directions)),                                        \n",
    "                     \"{} >nmod_of {} >nmod_with {}\".format(projection,namedx_np_f.format(advb),namedx_np_f.format(advb)),\n",
    "                     \"{} >nmod_of {} >nmod_with {}\".format(projection,namedx_np_f.format(directions),namedx_np_f.format(directions)),\n",
    "                     \"{} >nmod_of {} >nmod_with {}\".format(projection,case.format(advb,advb),namedx_np_f.format(advb)),\n",
    "                     \"{} >nmod_of {} >nmod_with {}\".format(projection,case.format(directions,directions),namedx_np_f.format(directions)),\n",
    "                     \"{} >nmod_of {} >nmod_with {}\".format(projection,namedx_np_f.format(advb),case.format(advb,advb)),\n",
    "                     \"{} >nmod_of {} >nmod_with {}\".format(projection,namedx_np_f.format(directions),case.format(directions,directions)),\n",
    "                     \"{} >nmod_of {} >nmod_with {}\".format(projection,case.format(advb,advb),case.format(advb,advb)),\n",
    "                     \"{} >nmod_of {} >nmod_with {}\".format(projection,case.format(directions,directions),case.format(directions,directions)),    \n",
    "                     \"{} [] [] [] [] {} >nmod_to {}\".format(projection,namedx_np_f.format(advb),namedx_np_f.format(advb)),\n",
    "                     \"{} [] [] [] [] {} >nmod_to {}\".format(projection,namedx_np_f.format(directions),namedx_np_f.format(directions)),\n",
    "                     \"{} [] [] [] [] {} >nmod_to {}\".format(projection,case.format(advb,advb),namedx_np_f.format(advb)),\n",
    "                     \"{} [] [] [] [] {} >nmod_to {}\".format(projection,case.format(directions,directions),namedx_np_f.format(directions)),\n",
    "                     \"{} [] [] [] [] {} >nmod_to {}\".format(projection,namedx_np_f.format(advb),case.format(advb,advb)),\n",
    "                     \"{} [] [] [] [] {} >nmod_to {}\".format(projection,namedx_np_f.format(directions),case.format(directions,directions)),\n",
    "                     \"{} [] [] [] [] {} >nmod_to {}\".format(projection,case.format(advb,advb),case.format(advb,advb)),\n",
    "                     \"{} [] [] [] [] {} >nmod_to {}\".format(projection,case.format(directions,directions),case.format(directions,directions)),               \n",
    "                     \"{} >nmod_to {} >nmod_from [] [] {}\".format(projection,namedx_np_f.format(advb),namedx_np_f.format(advb)),\n",
    "                     \"{} >nmod_to {} >nmod_from [] [] {}\".format(projection,namedx_np_f.format(directions),namedx_np_f.format(directions)),\n",
    "                     \"{} >nmod_to {} >nmod_from [] [] {}\".format(projection,case.format(advb,advb),namedx_np_f.format(advb)),\n",
    "                     \"{} >nmod_to {} >nmod_from [] [] {}\".format(projection,case.format(directions,directions),namedx_np_f.format(directions)),\n",
    "                     \"{} >nmod_to {} >nmod_from [] [] {}\".format(projection,namedx_np_f.format(advb),case.format(advb,advb)),\n",
    "                     \"{} >nmod_to {} >nmod_from [] [] {}\".format(projection,namedx_np_f.format(directions),case.format(directions,directions)),\n",
    "                     \"{} >nmod_to {} >nmod_from [] [] {}\".format(projection,case.format(advb,advb),case.format(advb,advb)),\n",
    "                     \"{} >nmod_to {} >nmod_from [] [] {}\".format(projection,case.format(directions,directions),case.format(directions,directions)),                                      \n",
    "                     \"{} [] [] [] {} >nmod_{} {}\".format(namedx_np_f.format(advb),projection,fromto,namedx_np_f.format(advb)),\n",
    "                     \"{} [] [] [] {} >nmod_{} {}\".format(namedx_np_f.format(directions),projection,fromto,namedx_np_f.format(directions)),\n",
    "                     \"{} [] [] [] {} >nmod_{} {}\".format(case.format(advb,advb),projection,fromto,namedx_np_f.format(advb)),\n",
    "                     \"{} [] [] [] {} >nmod_{} {}\".format(case.format(directions,directions),projection,fromto,namedx_np_f.format(directions)),\n",
    "                     \"{} [] [] [] {} >nmod_{} {}\".format(namedx_np_f.format(advb),projection,fromto,case.format(advb,advb)),\n",
    "                     \"{} [] [] [] {} >nmod_{} {}\".format(namedx_np_f.format(directions),projection,fromto,case.format(directions,directions)),\n",
    "                     \"{} [] [] [] {} >nmod_{} {}\".format(case.format(advb,advb),projection,fromto,case.format(advb,advb)),\n",
    "                     \"{} [] [] [] {} >nmod_{} {}\".format(case.format(directions,directions),projection,fromto,case.format(directions,directions)),                                     \n",
    "                     \"{} [lemma=afferent] >nmod_{} {}\".format(namedx_np_f.format(advb),fromto,namedx_np_f.format(advb)),\n",
    "                     \"{} [lemma=afferent] >nmod_{} {}\".format(namedx_np_f.format(directions),fromto,namedx_np_f.format(directions)),\n",
    "                     \"{} [lemma=afferent] >nmod_{} {}\".format(case.format(advb,advb),fromto,namedx_np_f.format(advb)),\n",
    "                     \"{} [lemma=afferent] >nmod_{} {}\".format(case.format(directions,directions),fromto,namedx_np_f.format(directions)),\n",
    "                     \"{} [lemma=afferent] >nmod_{} {}\".format(namedx_np_f.format(advb),fromto,case.format(advb,advb)),\n",
    "                     \"{} [lemma=afferent] >nmod_{} {}\".format(namedx_np_f.format(directions),fromto,case.format(directions,directions)),\n",
    "                     \"{} [lemma=afferent] >nmod_{} {}\".format(case.format(advb,advb),fromto,case.format(advb,advb)),\n",
    "                     \"{} [lemma=afferent] >nmod_{} {}\".format(case.format(directions,directions),fromto,case.format(directions,directions)),                                                      \n",
    "                     \"{} which [] [] {} >nmod_to {}\".format(namedx_np_f.format(advb),projection,namedx_np_f.format(advb)),\n",
    "                     \"{} which [] [] {} >nmod_to {}\".format(namedx_np_f.format(directions),projection,namedx_np_f.format(directions)),                   \n",
    "                     \"{} which [] [] {} >nmod_to {}\".format(case.format(advb,advb),projection,namedx_np_f.format(advb)),\n",
    "                     \"{} which [] [] {} >nmod_to {}\".format(case.format(directions,directions),projection,namedx_np_f.format(directions)),                      \n",
    "                     \"{} which [] [] {} >nmod_to {}\".format(namedx_np_f.format(advb),projection,case.format(advb,advb)),\n",
    "                     \"{} which [] [] {} >nmod_to {}\".format(namedx_np_f.format(directions),projection,case.format(directions,directions)),                     \n",
    "                     \"{} which [] [] {} >nmod_to {}\".format(case.format(advb,advb),projection,case.format(advb,advb)),\n",
    "                     \"{} which [] [] {} >nmod_to {}\".format(case.format(directions,directions),projection,case.format(directions,directions))]\n",
    "            for rule in rules:\n",
    "                addrule = rule\n",
    "                binary_rules.append(addrule)\n",
    "\n",
    "\n",
    "print(len(binary_rules))\n",
    "\n",
    "'''\n",
    "for rule in binary_rules:\n",
    "    print(rule)\n",
    "'''\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68e3cb87-fe15-4519-a4d7-11a399962cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[lemma=project] >nmod_from {}* (?<region> [chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])', '[lemma=connect] >nmod_from {}* (?<region> [chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])', '[lemma=pathway] >nmod_from {}* (?<region> [chunk=B-NP]|[chunk=I-NP]|[chunk=B-NP][chunk=I-NP])']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "projection_lemmas = [\"project\", \"connect\", \"pathway\"]\n",
    "\n",
    "# Construct individual rules for each lemma\n",
    "projection_rules = [\"[lemma={}] >nmod_from {}\".format(lemma, namedx_np_f) for lemma in projection_lemmas]\n",
    "print(projection_rules)\n",
    "\n",
    "# Combine the rules with logical OR\n",
    "combined_rule = \" | \".join(projection_rules)\n",
    "\n",
    "# Complete Odinson rule\n",
    "odinson_rule = \"{} {}\".format(namedx_np_f, combined_rule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef9c2247-79f1-4f67-ab35-bd06c1bfee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add in stopwords\n",
    "sw_nltk = stopwords.words('english')\n",
    "#print(sw_nltk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b860d51b-148f-41eb-ae04-b427bb787ba8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▊                                   | 216/2916 [00:00<00:02, 1145.49it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 17%|██████▎                               | 485/2916 [00:00<00:01, 1295.62it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 26%|█████████▉                            | 760/2916 [00:00<00:01, 1287.16it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 35%|█████████████                        | 1029/2916 [00:00<00:01, 1270.75it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 44%|████████████████▎                    | 1281/2916 [00:01<00:01, 1219.70it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 53%|███████████████████▋                 | 1547/2916 [00:01<00:01, 1261.61it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 62%|███████████████████████              | 1821/2916 [00:01<00:00, 1274.79it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 72%|██████████████████████████▌          | 2094/2916 [00:01<00:00, 1271.08it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 81%|█████████████████████████████▉       | 2361/2916 [00:01<00:00, 1253.80it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 90%|█████████████████████████████████▎   | 2623/2916 [00:02<00:00, 1224.90it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████████████████████████████████| 2916/2916 [00:02<00:00, 1213.64it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "relations = []\n",
    "readable_sentences = []\n",
    "#go through each rule and make it a rule object\n",
    "for rule_text in tqdm(binary_rules):\n",
    "    #print(rule_text)\n",
    "    rule = Rule(\"anatomical connection\", \"Exp\", \"basic\", rule_text)\n",
    "    #make sure it is a functional Odinson rule\n",
    "    try:\n",
    "        rule_output = process_rules([rule],\"http://localhost:9000\")\n",
    "        #print(rule_output)\n",
    "    except Exception as e:\n",
    "        print('failed', rule)\n",
    "        print(e)\n",
    "        continue\n",
    "    #get the start and end characters for each term pulled out by the rule\n",
    "    for sentence in rule_output['mentions']:\n",
    "        #print(sentence)\n",
    "        relation = ()\n",
    "        words = sentence['words']\n",
    "        string_words = ' '.join(words)\n",
    "        readable_sentences += [string_words]\n",
    "        for element in sentence['match']:  \n",
    "            for entity in element['namedCaptures']:\n",
    "                #print(entity)\n",
    "                start = entity['capturedMatch']['start']\n",
    "                end = entity['capturedMatch']['end']\n",
    "                #remove stop words\n",
    "                processed_term = [word for word in words[start:end] if word.lower() not in sw_nltk]\n",
    "                word = ' '.join(processed_term)\n",
    "                #create tuples with curies for terms that can be grounded\n",
    "                spine_scored_match = grounder.ground(word)\n",
    "                gilda_scored_match = gilda.ground(word)\n",
    "                if len(gilda_scored_match)>0:\n",
    "                    best_curie = gilda_scored_match[0].term.get_curie()\n",
    "                elif len(spine_scored_match)>0:\n",
    "                    best_curie = spine_scored_match[0].term.get_curie()\n",
    "                else:\n",
    "                    best_curie = None\n",
    "                relation += ((best_curie, word),)  \n",
    "        if len(relation) > 1:\n",
    "            relations.append(relation)\n",
    "\n",
    "print(len(relations))\n",
    "#print(readable_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fcb2441-6c14-4478-a697-bafeb671d263",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "set_1728 = []\n",
    "for sentence in readable_sentences:\n",
    "    set_1728 += [sentence]\n",
    "\n",
    "with open('all_sentences_1728.txt', 'a') as dfbig:\n",
    "    for sentence in readable_sentences:\n",
    "        dfbig.write(sentence + '\\n')\n",
    "'''\n",
    "set_5184 = []\n",
    "for sentence in readable_sentences:\n",
    "    set_5184 += [sentence]\n",
    "\n",
    "with open('all_sentences_5184.txt', 'a') as dfbig:\n",
    "    for sentence in readable_sentences:\n",
    "        dfbig.write(sentence + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "873493e2-7b1a-403a-b1ab-702a845ca409",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_in = []\n",
    "for sentence in set_5184:\n",
    "    if sentence not in set_1728:\n",
    "        not_in += [sentence]\n",
    "\n",
    "with open('not_in_big.txt', 'a') as file:\n",
    "    for sentence in not_in:\n",
    "        file.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0d57a-6c79-4579-bac4-4ca08b592c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#dfbig = pd.read_csv('all_sentencesbig.txt')\n",
    "#dfsmall = pd.read_csv('all_sentences_1728.txt')\n",
    "\n",
    "with open('all_sentencesbig.txt', 'r') as dfbig:\n",
    "    dfbig_text = dfbig.readlines()\n",
    "with open('all_sentences_1728.txt', 'r') as dfsmall:\n",
    "    dfsmall_text = dfsmall.readlines()    \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419dff32-2ad4-4d94-8e2e-08b950417f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_set = random.sample(readable_sentences,20)\n",
    "\n",
    "'''\n",
    "for sentence in sample_set:\n",
    "    print(sentence)\n",
    "'''\n",
    "\n",
    "with open('sample_20.txt', 'w') as f:\n",
    "    for sentence in sample_set:\n",
    "        f.write(sentence + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16e2cef-1162-4b86-8175-cf8830f6833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a ranked list of terms\n",
    "import csv\n",
    "'''with open('relations.csv', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerows(relations)'''\n",
    "terms = []\n",
    "for set in relations:\n",
    "    for term in set:\n",
    "        terms.append(term[1])\n",
    "\n",
    "        \n",
    "new_relations = np.array(terms)\n",
    "\n",
    "ranked = pd.value_counts(new_relations)\n",
    "\n",
    "value_counts_df = ranked.reset_index()\n",
    "value_counts_df.columns = ['Value', 'Count']\n",
    "\n",
    "with open('relations.csv', 'w') as f:\n",
    "    value_counts_df.to_csv(f, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7d4ba4-b281-405d-8359-fd7ebc5a9284",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('relation_full.csv', 'w', newline='') as f:    \n",
    "    csv_writer = csv.writer(f)\n",
    "\n",
    "    for relation1, relation2 in relations:\n",
    "        term_of_inner_tuple1 = relation1[1]\n",
    "        term_of_inner_tuple2 = relation2[1]\n",
    "        print(term_of_inner_tuple1,term_of_inner_tuple2)\n",
    "        csv_writer.writerow([term_of_inner_tuple1, term_of_inner_tuple2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3c3b7c-6612-47be-b4a6-82feeef90a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an interaction map of relationships between brain region terms\n",
    "#!pip install matplotlib\n",
    "import networkx as nx\n",
    "import pygraphviz as pgv\n",
    "import matplotlib.pyplot as plt\n",
    "G = nx.Graph()\n",
    "plt.figure(figsize=(20,20))\n",
    "G.add_edges_from(relations, len=4)\n",
    "\n",
    "pos = nx.nx_agraph.graphviz_layout(G, prog='neato')\n",
    "#labels = {n: n[1] for n in G.nodes()}\n",
    "labels = {}\n",
    "for k in pos.keys():\n",
    "    labels[k] = k[1]\n",
    "\n",
    "'''\n",
    "for label in labels.values():\n",
    "    textwrap.wrap(label, width = 10)\n",
    "    print(label)\n",
    "'''\n",
    "#G = nx.relabel_nodes(G, labels)\n",
    "#nx.draw_networkx_labels(G, pos, labels, font_size=22, font_color=\"black\")\n",
    "nx.draw_networkx_nodes(G, pos, node_size=100, node_color='white', node_shape='o')\n",
    "nx.draw_networkx_edges(G, pos, width=1.0, edge_color='grey', style='solid')\n",
    "labels = nx.draw_networkx_labels(G, pos, labels = labels, font_size=8, font_color='k', font_family='sans-serif', font_weight='normal')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab540d4-edef-4af1-9792-baeb1afd093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoredmatches = gilda.ground('ER')\n",
    "scoredmatches[0].term.get_curie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1eaf11-399c-4514-82f5-388f7e6b65ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_url = ('https://docs.google.com/spreadsheets/d/e/2PACX-1vS6uvih2Hi7dIo9Nabk5gv2kz67avmHpiWvqtNOKxrr43WhxSCBwzyq'\n",
    "'lLvi841Vx3f1LoF7GF_5Cff3/pub?output=tsv')\n",
    "benchmark_df = pd.read_csv(benchmark_url, sep='\\t')\n",
    "subject = gilda.ground_df(benchmark_df, 'subject', grounder=grounder)\n",
    "object = gilda.ground_df(benchmark_df, 'object', grounder=grounder)\n",
    "\n",
    "benchmark_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
